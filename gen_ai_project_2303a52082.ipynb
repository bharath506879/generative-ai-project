{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9146200,
          "sourceType": "datasetVersion",
          "datasetId": 5524489
        },
        {
          "sourceId": 9971597,
          "sourceType": "datasetVersion",
          "datasetId": 6134770
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "PqAm_TDjB92f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-11T08:28:31.105515Z",
          "iopub.execute_input": "2025-04-11T08:28:31.106016Z",
          "iopub.status.idle": "2025-04-11T08:28:35.637319Z",
          "shell.execute_reply.started": "2025-04-11T08:28:31.105989Z",
          "shell.execute_reply": "2025-04-11T08:28:35.636233Z"
        },
        "id": "V4XZ12I0B92h",
        "outputId": "57915fd7-262a-4679-c0e2-d9e53f39cc18"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.29.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-11T08:28:35.638569Z",
          "iopub.execute_input": "2025-04-11T08:28:35.638940Z",
          "iopub.status.idle": "2025-04-11T08:28:45.531534Z",
          "shell.execute_reply.started": "2025-04-11T08:28:35.638906Z",
          "shell.execute_reply": "2025-04-11T08:28:45.530878Z"
        },
        "id": "h9Ebzm-3B92h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for real and manipulated videos\n",
        "real_videos_dir = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\"\n",
        "manipulated_videos_dir = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\"\n",
        "\n",
        "# Output directories for extracted frames\n",
        "output_real_dir = \"/kaggle/working/frames/real\"\n",
        "output_manipulated_dir = \"/kaggle/working/frames/manipulated\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-11T08:28:45.533379Z",
          "iopub.execute_input": "2025-04-11T08:28:45.533621Z",
          "iopub.status.idle": "2025-04-11T08:28:45.537196Z",
          "shell.execute_reply.started": "2025-04-11T08:28:45.533600Z",
          "shell.execute_reply": "2025-04-11T08:28:45.536320Z"
        },
        "id": "uWFOel0IB92h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure output directories exist\n",
        "os.makedirs(output_real_dir, exist_ok=True)\n",
        "os.makedirs(output_manipulated_dir, exist_ok=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-11T08:28:45.538451Z",
          "iopub.execute_input": "2025-04-11T08:28:45.538679Z",
          "iopub.status.idle": "2025-04-11T08:28:45.671960Z",
          "shell.execute_reply.started": "2025-04-11T08:28:45.538660Z",
          "shell.execute_reply": "2025-04-11T08:28:45.671072Z"
        },
        "id": "JdgAs120B92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames_from_videos(videos_dir, output_dir, label, max_videos=50):\n",
        "    video_files = [f for f in os.listdir(videos_dir) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "    video_files = video_files[:max_videos]  # Limit to max_videos\n",
        "\n",
        "    for video_file in video_files:\n",
        "        video_path = os.path.join(videos_dir, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frame_count = 0\n",
        "        success, image = cap.read()\n",
        "\n",
        "        while success:\n",
        "            if frame_count % int(cap.get(cv2.CAP_PROP_FPS)) == 0:\n",
        "                frame_filename = f\"{label}_{video_file}_frame{frame_count // int(cap.get(cv2.CAP_PROP_FPS))}.jpg\"\n",
        "                frame_path = os.path.join(output_dir, frame_filename)\n",
        "                cv2.imwrite(frame_path, image)\n",
        "            success, image = cap.read()\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-11T08:28:45.672966Z",
          "iopub.execute_input": "2025-04-11T08:28:45.673315Z",
          "iopub.status.idle": "2025-04-11T08:28:45.683301Z",
          "shell.execute_reply.started": "2025-04-11T08:28:45.673279Z",
          "shell.execute_reply": "2025-04-11T08:28:45.682476Z"
        },
        "id": "o9Ea5AoMB92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract frames from 100 real and 100 manipulated videos\n",
        "extract_frames_from_videos(real_videos_dir, output_real_dir, \"real\", max_videos=100)\n",
        "extract_frames_from_videos(manipulated_videos_dir, output_manipulated_dir, \"manipulated\", max_videos=100)\n",
        "print(\"Frame extraction completed.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-11T08:28:45.684076Z",
          "iopub.execute_input": "2025-04-11T08:28:45.684313Z"
        },
        "id": "Hrsyv7C_B92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define image transformations with advanced augmentations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "trusted": true,
        "id": "udIpc7qDB92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Custom Dataset Class\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.class_to_idx = {'manipulated': 0, 'real': 1}\n",
        "\n",
        "        for class_name in ['manipulated', 'real']:\n",
        "            class_path = self.root_dir / class_name\n",
        "            if not class_path.exists():\n",
        "                continue\n",
        "            for img_path in class_path.rglob(\"*.*\"):\n",
        "                if img_path.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']:\n",
        "                    self.images.append(img_path)\n",
        "                    self.labels.append(self.class_to_idx[class_name])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.images[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Dataset Directory\n",
        "dataset_dir = \"/kaggle/working/frames\"\n",
        "\n",
        "# Load Dataset\n",
        "dataset = CustomImageDataset(root_dir=dataset_dir, transform=transform)\n",
        "\n",
        "# Check if images are loaded\n",
        "print(\"Total images loaded:\", len(dataset))\n",
        "\n",
        "# Debugging Counts\n",
        "print(\"Manipulated Images:\", len(list((Path(dataset_dir) / 'manipulated').rglob('*.*'))))\n",
        "print(\"Real Images:\", len(list((Path(dataset_dir) / 'real').rglob('*.*'))))\n",
        "\n",
        "# Split Dataset\n",
        "if len(dataset) == 0:\n",
        "    raise ValueError(\"No images found. Check dataset directory structure.\")\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "print(\"Train Size:\", len(train_dataset))\n",
        "print(\"Validation Size:\", len(val_dataset))\n",
        "\n",
        "# Verify sample\n",
        "img, label = dataset[0]\n",
        "print(\"First Image Label:\", label)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "bWj7HHA4B92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Vision Transformer (ViT) model\n",
        "model = timm.create_model('vit_large_patch16_224', pretrained=True, num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
        "\n",
        "# Training loop with early stopping\n",
        "num_epochs = 20\n",
        "best_val_accuracy = 0\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct_val / total_val\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Save best model with early stopping\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        torch.save(model.state_dict(), 'best_vit_model.pth')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping due to no improvement.\")\n",
        "            break\n",
        "    scheduler.step()\n",
        "\n",
        "print(f\"Best Validation Accuracy: {best_val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Sr0dGdq9B92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "def plot_auc_roc(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_labels, all_probs = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)[:, 1]  # Probability of class 1\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, label=f'AUC = {auc(fpr, tpr):.4f}', color='blue', lw=2)\n",
        "    plt.plot([0, 1], [0, 1], 'k--')  # Random classifier line\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "plot_auc_roc(model, val_loader, device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dCoI0-vbB92i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "\n",
        "# Assuming you have predicted probabilities and true labels\n",
        "y_true = []  # List to store true labels\n",
        "y_scores = []  # List to store predicted probabilities\n",
        "\n",
        "# Get model predictions (assuming `val_loader` is your validation dataloader)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]  # Get probability for positive class\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_scores.extend(probs.cpu().numpy())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_scores = np.array(y_scores)\n",
        "\n",
        "# ===== ROC Curve =====\n",
        "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, color=\"blue\", lw=2, label=f\"AUC = {roc_auc:.4f}\")\n",
        "plt.plot([0, 1], [0, 1], color=\"black\", linestyle=\"--\")  # Random classifier\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# ===== Precision-Recall Curve =====\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
        "pr_auc = average_precision_score(y_true, y_scores)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(recall, precision, color=\"red\", lw=2, label=f\"AP = {pr_auc:.4f}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "YediHk5eB92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "FileLink('best_vit_model.pth')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1IxTfQhhB92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_vit_model.pth', weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate classification metrics\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(all_labels, all_predictions, target_names=['Real', 'Manipulated']))\n",
        "\n",
        "# Optionally print accuracy separately\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5NkWT2y-B92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Manipulated'], yticklabels=['Real', 'Manipulated'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "noJbhIs4B92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import timm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define image transformations (same as used during training)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the trained model\n",
        "model = timm.create_model('vit_large_patch16_224', pretrained=False, num_classes=2)\n",
        "# Load the trained model with weights_only=True for security\n",
        "model.load_state_dict(torch.load('best_vit_model.pth', weights_only=True))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Function to process the video and classify each frame\n",
        "def predict_video(video_path, model, transform, device):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = 0\n",
        "    real_count = 0\n",
        "    manipulated_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "        # Convert frame to PIL Image and apply transformations\n",
        "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
        "\n",
        "        # Make prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        if predicted.item() == 0:\n",
        "            real_count += 1\n",
        "        else:\n",
        "            manipulated_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Final decision based on majority vote across all frames\n",
        "    if real_count > manipulated_count:\n",
        "        print(f\"Result: Real video ({real_count} real frames, {manipulated_count} manipulated frames)\")\n",
        "        return \"Real\"\n",
        "    else:\n",
        "        print(f\"Result: Manipulated video ({real_count} real frames, {manipulated_count} manipulated frames)\")\n",
        "        return \"Manipulated\"\n",
        "\n",
        "# Test the video\n",
        "video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences/02__kitchen_still.mp4\"\n",
        "result = predict_video(video_path, model, transform, device)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "c9vC5ECMB92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences/01_20__walking_and_outside_surprised__OTGHOG4Z.mp4\"\n",
        "result = predict_video(video_path, model, transform, device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ROX8YgHMB92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences/15__exit_phone_room.mp4\"\n",
        "result = predict_video(video_path, model, transform, device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WBCeAkNTB92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences/01_20__walking_and_outside_surprised__OTGHOG4Z.mp4\"\n",
        "result = predict_video(video_path, model, transform, device)"
      ],
      "metadata": {
        "trusted": true,
        "id": "aAxVckNIB92j"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Wlt1Qm9LB92j"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}